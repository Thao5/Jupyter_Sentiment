{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d179a1a-ea38-4473-aed0-2ca8b74d878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting underthesea==1.2.2a0\n",
      "  Downloading underthesea-1.2.2a0-py3-none-any.whl (7.5 MB)\n",
      "                                              0.0/7.5 MB ? eta -:--:--\n",
      "                                              0.1/7.5 MB 1.9 MB/s eta 0:00:04\n",
      "     -                                        0.2/7.5 MB 2.5 MB/s eta 0:00:03\n",
      "     --                                       0.4/7.5 MB 3.2 MB/s eta 0:00:03\n",
      "     ---                                      0.7/7.5 MB 4.2 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     -----                                    1.0/7.5 MB 5.1 MB/s eta 0:00:02\n",
      "     ------                                   1.2/7.5 MB 1.6 MB/s eta 0:00:04\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------                              2.1/7.5 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------                          2.9/7.5 MB 2.2 MB/s eta 0:00:03\n",
      "     ----------------                         3.1/7.5 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------                         3.1/7.5 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------                         3.1/7.5 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------                         3.1/7.5 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------                         3.1/7.5 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------                         3.1/7.5 MB 2.3 MB/s eta 0:00:02\n",
      "     ------------------                       3.4/7.5 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------                   4.2/7.5 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------                   4.2/7.5 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------                   4.2/7.5 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------                   4.2/7.5 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------                   4.2/7.5 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------                   4.2/7.5 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------                   4.2/7.5 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------                 4.6/7.5 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------------------             5.2/7.5 MB 2.5 MB/s eta 0:00:01\n",
      "     ----------------------------             5.2/7.5 MB 2.5 MB/s eta 0:00:01\n",
      "     ----------------------------             5.2/7.5 MB 2.5 MB/s eta 0:00:01\n",
      "     ----------------------------             5.2/7.5 MB 2.5 MB/s eta 0:00:01\n",
      "     ----------------------------             5.2/7.5 MB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------------          6.0/7.5 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        6.3/7.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------------     6.7/7.5 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.3/7.5 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.3/7.5 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.3/7.5 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.3/7.5 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.5/7.5 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.5/7.5 MB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Click>=6.0 in d:\\jupyter_proj\\venv\\lib\\site-packages (from underthesea==1.2.2a0) (8.1.7)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in d:\\jupyter_proj\\venv\\lib\\site-packages (from underthesea==1.2.2a0) (0.9.10)\n",
      "Collecting nltk<3.5,>=3.4 (from underthesea==1.2.2a0)\n",
      "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     -----------------                        0.6/1.5 MB 20.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.4/1.5 MB 22.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 18.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting tabulate (from underthesea==1.2.2a0)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: tqdm in d:\\jupyter_proj\\venv\\lib\\site-packages (from underthesea==1.2.2a0) (4.66.2)\n",
      "Requirement already satisfied: requests in d:\\jupyter_proj\\venv\\lib\\site-packages (from underthesea==1.2.2a0) (2.31.0)\n",
      "Requirement already satisfied: joblib in d:\\jupyter_proj\\venv\\lib\\site-packages (from underthesea==1.2.2a0) (1.3.2)\n",
      "Collecting scikit-learn<0.22,>=0.20 (from underthesea==1.2.2a0)\n",
      "  Downloading scikit-learn-0.21.3.tar.gz (12.2 MB)\n",
      "                                              0.0/12.2 MB ? eta -:--:--\n",
      "     ---                                      1.0/12.2 MB 31.0 MB/s eta 0:00:01\n",
      "     ------                                   2.1/12.2 MB 25.9 MB/s eta 0:00:01\n",
      "     ----------                               3.3/12.2 MB 26.3 MB/s eta 0:00:01\n",
      "     --------------                           4.5/12.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------                       5.7/12.2 MB 26.1 MB/s eta 0:00:01\n",
      "     ----------------------                   6.8/12.2 MB 25.5 MB/s eta 0:00:01\n",
      "     --------------------------               8.0/12.2 MB 25.7 MB/s eta 0:00:01\n",
      "     ------------------------------           9.2/12.2 MB 25.5 MB/s eta 0:00:01\n",
      "     -------------------------------          9.6/12.2 MB 23.5 MB/s eta 0:00:01\n",
      "     ---------------------------------       10.6/12.2 MB 23.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   11.8/12.2 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.2/12.2 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.2/12.2 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.2/12.2 MB 18.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [26 lines of output]\n",
      "  <string>:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  Partial import of sklearn during the build process.\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 187, in get_numpy_status\n",
      "  ModuleNotFoundError: No module named 'numpy'\n",
      "  Traceback (most recent call last):\n",
      "    File \"D:\\jupyter_proj\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "      main()\n",
      "    File \"D:\\jupyter_proj\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"D:\\jupyter_proj\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n",
      "      return hook(metadata_directory, config_settings)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Chung Vu\\AppData\\Local\\Temp\\pip-build-env-2k9blr4c\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 366, in prepare_metadata_for_build_wheel\n",
      "      self.run_setup()\n",
      "    File \"C:\\Users\\Chung Vu\\AppData\\Local\\Temp\\pip-build-env-2k9blr4c\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 487, in run_setup\n",
      "      super().run_setup(setup_script=setup_script)\n",
      "    File \"C:\\Users\\Chung Vu\\AppData\\Local\\Temp\\pip-build-env-2k9blr4c\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\n",
      "      exec(code, locals())\n",
      "    File \"<string>\", line 290, in <module>\n",
      "    File \"<string>\", line 278, in setup_package\n",
      "  ImportError: Numerical Python (NumPy) is not installed.\n",
      "  scikit-learn requires NumPy >= 1.11.0.\n",
      "  Installation instructions are available on the scikit-learn website: http://scikit-learn.org/stable/install.html\n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: D:\\jupyter_proj\\venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# %pip install -r requirements.txt\n",
    "%pip install underthesea==1.2.2a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4357e819-9dc5-49d9-8140-a17b0038b7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.87      0.86      2313\n",
      "     neutral       0.70      0.36      0.48        97\n",
      "    positive       0.86      0.86      0.86      2590\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.80      0.70      0.73      5000\n",
      "weighted avg       0.85      0.85      0.85      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.1}\n",
      "Validation Accuracy with Best Model: 0.8486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.86      0.86      2313\n",
      "     neutral       1.00      0.05      0.10        97\n",
      "    positive       0.85      0.86      0.86      2590\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.90      0.59      0.60      5000\n",
      "weighted avg       0.85      0.85      0.84      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\jupyter_proj\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import base64\n",
    "import decimal\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import spark\n",
    "\n",
    "import flask\n",
    "# import pymysql\n",
    "import requests\n",
    "from flask import Flask, render_template, request\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocess_kgptalkie as ps\n",
    "import re\n",
    "from flask_cors import CORS\n",
    "import glob\n",
    "import os\n",
    "from underthesea import sentiment, pos_tag, word_tokenize\n",
    "from flask import g\n",
    "from sklearn.pipeline import Pipeline\n",
    "import underthesea\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pandas.errors import ParserError\n",
    "import csv\n",
    "\n",
    "\n",
    "SIZE_DEMO = 5000\n",
    "\n",
    "root = '/jupyter_proj/dataset/**/'\n",
    "# print(all_files)\n",
    "\n",
    "\n",
    "def get_clean(x):\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', ' ')\n",
    "    # x = str(x).lower()\n",
    "    x = ps.cont_exp(x)\n",
    "    x = ps.remove_emails(x)\n",
    "    x = ps.remove_urls(x)\n",
    "    x = ps.remove_html_tags(x)\n",
    "    # x = ps.remove_accented_chars(x)\n",
    "    # x = ps.remove_special_chars(x)\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    return x\n",
    "\n",
    "def load_data_train_to_excel(root):\n",
    "    data_train = pd.DataFrame()\n",
    "    all_files_train = glob.glob(root + \"train/**/*.txt\", recursive=True)\n",
    "    for f in all_files_train:     \n",
    "        df = pd.read_csv(f, header=None, sep = ' ', names = ['Review','Sentiment'], on_bad_lines='skip')\n",
    "        with open(f, mode='r', encoding=\"utf8\") as f:\n",
    "            df['Review'] = get_clean(f.read())\n",
    "            df['Sentiment'] = \"\"\n",
    "        data_train = pd.concat([data_train, df], ignore_index=True) \n",
    "        # print(data)\n",
    "    data_train.to_csv('data_train.csv', index=False)\n",
    "\n",
    "# i=-1\n",
    "# all_files_test = glob.glob(f'{root}test/{i}/*.txt', recursive=True)\n",
    "# for f in all_files_test:     \n",
    "#     df = pd.read_csv(f, header=None, sep = ' ', names = ['Review','Sentiment'], on_bad_lines='skip')\n",
    "#     with open(f, mode='r', encoding=\"utf8\") as f:\n",
    "#         df['Review'] = get_clean(f.read())\n",
    "#         df['Sentiment'] = \"negative\"\n",
    "#     data_test = pd.concat([data_test, df], ignore_index=True) \n",
    "#     print(data_test)\n",
    "\n",
    "\n",
    "def load_data_test_to_excel(root):\n",
    "    data_test = pd.DataFrame()\n",
    "    sentiment_array = [\"negative\", \"positive\", \"neutral\"]\n",
    "    all_files_test = []\n",
    "    for i in [-1,1,2]:\n",
    "        all_files_test = glob.glob(f'{root}test/{i}/*.txt', recursive=True)\n",
    "        print(i)\n",
    "        for f in all_files_test:\n",
    "            df = pd.read_csv(f, header=None, sep = ' ', names = ['Review','Sentiment'], on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n",
    "            with open(f, mode='r', encoding=\"utf8\") as f:\n",
    "                df['Review'] = get_clean(f.read())\n",
    "                df['Sentiment'] = sentiment_array[(i+1) if i==-1 else i]\n",
    "            data_test = pd.concat([data_test, df], ignore_index=True) \n",
    "            # print(data_test\n",
    "    data_test.to_csv('data_test.csv', index=False)\n",
    "\n",
    "\n",
    "# load_data_train_to_excel(root)\n",
    "# load_data_test_to_excel(root)\n",
    "\n",
    "df = pd.read_csv('data_test.csv')\n",
    "df_train = pd.read_csv('data_train.csv')\n",
    "\n",
    "\n",
    "# # existing_file = 'Book2.csv'\n",
    "# # df = pd.read_csv(existing_file)\n",
    "# # df['Review'] = df['Review'].apply(lambda x: get_clean(x))\n",
    "\n",
    "# # total_files = len(df)\n",
    "# # train_size = int(0.6 * total_files)\n",
    "# # val_size = int(0.2 * total_files)\n",
    "# # test_size = total_files - train_size - val_size\n",
    "\n",
    "# # train_files = df[:train_size]\n",
    "# # val_files = df[train_size:train_size + val_size]\n",
    "# # test_files = df[train_size + val_size:]\n",
    "\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Review'], df['Sentiment'], test_size=0.3, random_state=0)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "def preprocess(docs):\n",
    "    return [underthesea.word_tokenize(doc.lower()) for doc in docs]\n",
    "\n",
    "\n",
    "def get_vocabularies(tokenized_docs):\n",
    "    vocabs = set()\n",
    "    for doc in tokenized_docs:\n",
    "        vocabs.update(doc)\n",
    "    return vocabs\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "# def create_vector(tokenized_docs):\n",
    "    \n",
    "\n",
    "# tokenized_docs = preprocess(train_texts[:SIZE_DEMO].astype('U'))\n",
    "# tokenized_docs_val = preprocess(val_texts[:SIZE_DEMO].astype('U'))\n",
    "# tokenized_docs_test = preprocess(test_texts[:SIZE_DEMO].astype('U'))\n",
    "# vocabs = get_vocabularies(tokenized_docs)\n",
    "# print(tokenized_docs)\n",
    "tokenized_docs = train_texts[:SIZE_DEMO].astype('U')\n",
    "tokenized_docs_val = val_texts[:SIZE_DEMO].astype('U')\n",
    "tokenized_docs_test = test_texts[:SIZE_DEMO].astype('U')\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "# tfidf = TfidfVectorizer(analyzer=\"word\", tokenizer=identity_tokenizer, token_pattern=None, lowercase=False)\n",
    "tfidf = TfidfVectorizer(analyzer=\"word\", tokenizer=underthesea.word_tokenize, token_pattern=None, lowercase=False)\n",
    "# print(tokenized_docs[:100])\n",
    "X_train = tfidf.fit_transform(tokenized_docs)\n",
    "# print(X_train)\n",
    "X_train.shape\n",
    "y_train = train_labels[:SIZE_DEMO]\n",
    "# X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2,\n",
    "#                                                                                 random_state=0)\n",
    "clf = LinearSVC().fit(X_train, y_train)\n",
    "# clf = tree.DecisionTreeClassifier().fit(X_train, y_train)\n",
    "# clf = MultinomialNB().fit(X_train, y_train)\n",
    "# clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "def val_process(tfidf, clf):\n",
    "    X_val = tfidf.transform(tokenized_docs_val)\n",
    "    val_predictions = clf.predict(X_val)\n",
    "    val_accuracy = accuracy_score(val_labels[:SIZE_DEMO], val_predictions)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(classification_report(val_labels[:SIZE_DEMO], val_predictions))\n",
    "    \n",
    "    # docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "    # X_new_tfidf = tfidf.transform(docs_new)\n",
    "    \n",
    "    # y_pred = clf.predict(X_new_tfidf)\n",
    "    \n",
    "    # for doc, category in zip(docs_new, y_pred):\n",
    "    #     print('%r => %s' % (doc, category))\n",
    "        \n",
    "    # X_new_tfidf = tfidf.transform(test_texts)\n",
    "    \n",
    "    # predicted = clf.predict(X_new_tfidf)\n",
    "    \n",
    "    # np.mean(predicted == test_labels)\n",
    "    \n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        # 'max_iter': [1000, 5000, 10000],\n",
    "    }\n",
    "    \n",
    "    # tree_param = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n",
    "    \n",
    "    # parameters = {  \n",
    "    #     'alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)  \n",
    "    # }  \n",
    "    \n",
    "    # parameters = {  \n",
    "    #     'max_depth':[3,5,10,None],\n",
    "    #               'n_estimators':[10,100,200],\n",
    "    #               'max_features':[1,3,5,7],\n",
    "    #               'min_samples_leaf':[1,2,3],\n",
    "    #               'min_samples_split':[1,2,3]\n",
    "    # }  \n",
    "    \n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(LinearSVC(), param_grid, cv=3, refit = True, scoring='accuracy')\n",
    "    \n",
    "    # grid_search = GridSearchCV(tree.DecisionTreeClassifier(), tree_param, cv=3, refit = True, scoring='accuracy')\n",
    "    \n",
    "    # grid_search = GridSearchCV(MultinomialNB(), parameters, cv=3, refit = True, scoring='accuracy')\n",
    "    \n",
    "    # grid_search = GridSearchCV(RandomForestClassifier(), parameters, cv=3, refit = True, scoring='accuracy')\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train, train_labels[:SIZE_DEMO])\n",
    "    \n",
    "    # Best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    \n",
    "    # Evaluate best model on validation set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    val_predictions = best_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(val_labels[:SIZE_DEMO], val_predictions)\n",
    "    print(f\"Validation Accuracy with Best Model: {val_accuracy:.4f}\")\n",
    "    print(classification_report(val_labels[:SIZE_DEMO], val_predictions))\n",
    "\n",
    "\n",
    "val_process(tfidf, clf)\n",
    "\n",
    "\n",
    "def train_demo(df_train, tfidf, clf):\n",
    "    for index, row in df_train.iterrows():\n",
    "        X_val = tfidf.transform([row['Review']])\n",
    "        row['Sentiment'] = str(clf.predict(X_val)).strip('[]\\'')\n",
    "        print(f\"{row['Review']}=>{row['Sentiment']}\")\n",
    "\n",
    "\n",
    "# train_demo(df_train, tfidf, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e1f73-e847-4a41-a1a0-84f0ab278218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70c580-c69e-4511-b9a8-a013adb44e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
